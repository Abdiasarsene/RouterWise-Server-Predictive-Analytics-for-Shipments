# main.py
from fastapi import FastAPI
from app.config import Settings
from app.model_loader import load_mlflow_model, load_bentoml_model
import logging

app = FastAPI()

settings = Settings()
model = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@app.on_event("startup")
async def startup_event():
    global model
    logger.info("ğŸš€ DÃ©marrage de l'API...")
    print("ğŸš€ DÃ©marrage de l'API...")

    if settings.mlflow:
        logger.info("ğŸ§  Chargement du modÃ¨le MLflow...")
        model = load_mlflow_model(settings.mlflow)
    elif settings.bentoml:
        logger.info("ğŸ§  Chargement du modÃ¨le BentoML...")
        model = load_bentoml_model(settings.bentoml)
    else:
        logger.error("âŒ Aucun modÃ¨le nâ€™a Ã©tÃ© spÃ©cifiÃ© dans les variables dâ€™environnement.")
        raise RuntimeError("âŒ Aucun modÃ¨le nâ€™a Ã©tÃ© spÃ©cifiÃ©.")

    logger.info("âœ… ModÃ¨le chargÃ© avec succÃ¨s")
    print("âœ… ModÃ¨le chargÃ© avec succÃ¨s")

@app.get("/")
def root():
    print("âœ… Endpoint racine appelÃ©")
    return {"message": "L'API SupplyChain est opÃ©rationnelle."}

@app.get("/test-model")
def test_model():
    if model is None:
        raise RuntimeError("âŒ ModÃ¨le non chargÃ©.")
    return {"status": "ModÃ¨le disponible et chargÃ©"}
